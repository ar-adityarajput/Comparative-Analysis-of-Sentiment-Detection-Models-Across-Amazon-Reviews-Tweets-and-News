{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_absolute_error, r2_score\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load datasets\n",
    "tweets_df = pd.read_csv(\"Tweets.csv\")\n",
    "news_df = pd.read_csv(\"News.csv\", encoding=\"ISO-8859-1\")\n",
    "news_df.columns = ['Sentiment', 'Text']\n",
    "amazon_review_df = pd.read_csv(\"Amazone review.csv\")\n",
    "amazon_review_df.columns = ['Sentiment','Title', 'Text']\n",
    "amazon_review_df = amazon_review_df.drop(['Title'], axis=1)\n",
    "tweets_df = tweets_df.drop(['textID','selected_text'], axis=1)\n",
    "tweets_df = tweets_df.rename(columns={'text': 'Text','sentiment':'Sentiment'})\n",
    "\n",
    "amazon_review_df = amazon_review_df.dropna()\n",
    "positive_reviews = amazon_review_df[amazon_review_df['Sentiment'] == 2].sample(5000, random_state=42)\n",
    "negative_reviews = amazon_review_df[amazon_review_df['Sentiment'] == 1].sample(5000, random_state=42)\n",
    "# Combine positive and negative reviews\n",
    "amazon_review_df = pd.concat([positive_reviews, negative_reviews])\n",
    "# Shuffle the rows\n",
    "amazon_review_df = amazon_review_df.sample(frac=1, random_state=42)\n",
    "# amazon_review_df['Sentiment'] = amazon_review_df['Sentiment'].replace({2 :'positive', 1 : 'negative'})\n",
    "news_df['Sentiment'] = news_df['Sentiment'].replace({'positive': 2, 'negative': 1, 'neutral': 0})\n",
    "tweets_df['Sentiment'] = tweets_df['Sentiment'].replace({'positive': 2, 'negative': 1,'neutral': 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing remove stopwords symbols , cleaning the data.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('{html}|<.*?>|http\\S+|\\$\\w+[,]|\\$\\w+|[,]\\$\\w+|[0-9]+', '', text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "news_df['Processed_Text'] = news_df['Text'].apply(preprocess)\n",
    "amazon_review_df['Processed_Text'] = amazon_review_df['Text'].apply(preprocess)\n",
    "tweets_df['Processed_Text'] = tweets_df['Text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "Accuracy: 0.617\n",
      "Mean Absolute Error: 0.5590568372025325\n",
      "R^2 Score: -0.2914065745909489\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.79      0.64      5599\n",
      "           1       0.77      0.39      0.52      3914\n",
      "           2       0.71      0.60      0.65      4228\n",
      "\n",
      "    accuracy                           0.62     13741\n",
      "   macro avg       0.67      0.59      0.60     13741\n",
      "weighted avg       0.66      0.62      0.61     13741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:25<00:12, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM\n",
      "Accuracy: 0.679\n",
      "Mean Absolute Error: 0.45964631395094974\n",
      "R^2 Score: -0.043940299549467765\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.82      0.69      5599\n",
      "           1       0.78      0.50      0.61      3914\n",
      "           2       0.78      0.67      0.72      4228\n",
      "\n",
      "    accuracy                           0.68     13741\n",
      "   macro avg       0.72      0.66      0.67     13741\n",
      "weighted avg       0.71      0.68      0.68     13741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.691\n",
      "Mean Absolute Error: 0.4365038934575358\n",
      "R^2 Score: 0.018803443246704354\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69      5599\n",
      "           1       0.74      0.56      0.64      3914\n",
      "           2       0.73      0.75      0.74      4228\n",
      "\n",
      "    accuracy                           0.69     13741\n",
      "   macro avg       0.70      0.68      0.69     13741\n",
      "weighted avg       0.70      0.69      0.69     13741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Model used Naive Bayes, SVM, Random Forest On Tweets Data \n",
    "\n",
    "X = tweets_df['Processed_Text'] \n",
    "y = tweets_df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    # etc for deep learning models\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in tqdm(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# Models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "Accuracy: 0.811\n",
      "Mean Absolute Error: 0.1885\n",
      "R^2 Score: 0.24587255246136586\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.85      0.82       987\n",
      "           2       0.84      0.78      0.81      1013\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:16<00:08,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM\n",
      "Accuracy: 0.835\n",
      "Mean Absolute Error: 0.165\n",
      "R^2 Score: 0.3398884411465537\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.84      0.83       987\n",
      "           2       0.84      0.83      0.84      1013\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.83      0.84      0.83      2000\n",
      "weighted avg       0.84      0.83      0.84      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:21<00:00,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.812\n",
      "Mean Absolute Error: 0.1875\n",
      "R^2 Score: 0.2498732285756292\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.81      0.81       987\n",
      "           2       0.82      0.81      0.81      1013\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = amazon_review_df['Processed_Text'] \n",
    "y = amazon_review_df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    # etc for deep learning models\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in tqdm(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# Models\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "Accuracy: 0.674\n",
      "Mean Absolute Error: 0.543859649122807\n",
      "R^2 Score: -0.22753849152212036\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.96      0.80       567\n",
      "           1       1.00      0.09      0.16       115\n",
      "           2       0.63      0.34      0.44       287\n",
      "\n",
      "    accuracy                           0.67       969\n",
      "   macro avg       0.77      0.46      0.47       969\n",
      "weighted avg       0.70      0.67      0.62       969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM\n",
      "Accuracy: 0.716\n",
      "Mean Absolute Error: 0.47678018575851394\n",
      "R^2 Score: -0.08137215902264772\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.96      0.81       567\n",
      "           1       0.74      0.37      0.49       115\n",
      "           2       0.78      0.37      0.50       287\n",
      "\n",
      "    accuracy                           0.72       969\n",
      "   macro avg       0.74      0.57      0.60       969\n",
      "weighted avg       0.73      0.72      0.68       969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.731\n",
      "Mean Absolute Error: 0.45304437564499483\n",
      "R^2 Score: -0.028338356965316835\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.81       567\n",
      "           1       0.78      0.39      0.52       115\n",
      "           2       0.76      0.45      0.57       287\n",
      "\n",
      "    accuracy                           0.73       969\n",
      "   macro avg       0.75      0.59      0.63       969\n",
      "weighted avg       0.74      0.73      0.71       969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = news_df['Processed_Text'] \n",
    "y = news_df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    # etc for deep learning models\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in tqdm(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# Models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For LSTM:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# For BERT:\n",
    "\n",
    "\n",
    "# Tokenization and Padding\n",
    "MAX_LEN = 150\n",
    "VOCAB_SIZE = 10000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(news_df['Processed_Text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(news_df['Processed_Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "\n",
    "# LSTM model\n",
    "model_lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64, input_length=MAX_LEN),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n",
    "X = padded_sequences\n",
    "y = news_df['Sentiment'].values\n",
    "\n",
    "model_lstm.fit(X, y, epochs=5, validation_split=0.2, batch_size=32)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, classification_report\n",
    "\n",
    "# Predictions\n",
    "with tf.device('/CPU:0'):\n",
    "    y_pred_lstm = model_lstm.predict(X_test).flatten()\n",
    "y_pred_class = [1 if i > 0.5 else 0 for i in y_pred_lstm]\n",
    "\n",
    "# Metrics\n",
    "accuracy = sum(y_pred_class == y_test) / len(y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_class)\n",
    "r2 = r2_score(y_test, y_pred_class)\n",
    "class_report = classification_report(y_test, y_pred_class)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "388/388 [==============================] - 1374s 3s/step - loss: 0.7150 - accuracy: 0.6897 - val_loss: 0.5295 - val_accuracy: 0.7835\n",
      "Epoch 2/2\n",
      "388/388 [==============================] - 1101s 3s/step - loss: 0.3998 - accuracy: 0.8487 - val_loss: 0.4394 - val_accuracy: 0.8299\n",
      "31/31 [==============================] - 79s 2s/step\n",
      "Accuracy: 0.803921568627451\n",
      "Mean Absolute Error: 0.31475748194014447\n",
      "R^2 Score: 0.3079735585201955\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       567\n",
      "           1       0.69      0.63      0.66       115\n",
      "           2       0.74      0.76      0.75       287\n",
      "\n",
      "    accuracy                           0.80       969\n",
      "   macro avg       0.76      0.75      0.76       969\n",
      "weighted avg       0.80      0.80      0.80       969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Change number of labels to 3\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in news_df['Processed_Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True,\n",
    "                                         return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = [t.numpy()[0] for t in input_ids]\n",
    "attention_masks = [t.numpy()[0] for t in attention_masks]\n",
    "\n",
    "X_ids = tf.convert_to_tensor(input_ids)\n",
    "X_masks = tf.convert_to_tensor(attention_masks)\n",
    "y = tf.convert_to_tensor(news_df['Sentiment'].values)\n",
    "X_ids_np = X_ids.numpy()\n",
    "X_masks_np = X_masks.numpy()\n",
    "y_np = y.numpy() \n",
    "\n",
    "X_ids_train, X_ids_test, X_masks_train, X_masks_test, y_train, y_test = train_test_split(X_ids_np, X_masks_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # Change to CategoricalCrossentropy\n",
    "metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')  # Change metric for multiclass\n",
    "\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "y_one_hot_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "model_bert.fit([X_ids_train, X_masks_train], y_one_hot_train, epochs=2, validation_split=0.2, batch_size=8)\n",
    "\n",
    "# Predictions\n",
    "y_pred_bert = model_bert.predict([X_ids_test, X_masks_test])[0]\n",
    "y_pred_bert_class = np.argmax(y_pred_bert, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = sum(y_pred_bert_class == y_test) / len(y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_bert_class)\n",
    "r2 = r2_score(y_test, y_pred_bert_class)\n",
    "class_report = classification_report(y_test, y_pred_bert_class)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"C:/Users/adity/OneDrive/Jigar bhai/Jigar Final Project/Code\" \n",
    "model_bert.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 150, 64)           640000    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 150, 64)           33024     \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 685,473\n",
      "Trainable params: 685,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "687/687 [==============================] - 44s 60ms/step - loss: 0.3024 - accuracy: 0.2818 - val_loss: 0.3569 - val_accuracy: 0.2885\n",
      "Epoch 2/5\n",
      "687/687 [==============================] - 41s 60ms/step - loss: 0.2973 - accuracy: 0.2818 - val_loss: 0.3564 - val_accuracy: 0.2885\n",
      "Epoch 3/5\n",
      "687/687 [==============================] - 42s 61ms/step - loss: 0.2971 - accuracy: 0.2818 - val_loss: 0.3556 - val_accuracy: 0.2885\n",
      "Epoch 4/5\n",
      "687/687 [==============================] - 42s 61ms/step - loss: 0.2965 - accuracy: 0.2818 - val_loss: 0.3596 - val_accuracy: 0.2885\n",
      "Epoch 5/5\n",
      "687/687 [==============================] - 40s 58ms/step - loss: 0.2965 - accuracy: 0.2818 - val_loss: 0.3592 - val_accuracy: 0.2885\n",
      "172/172 [==============================] - 4s 21ms/step\n",
      "Accuracy: 0.28415499363289065\n",
      "Mean Absolute Error: 0.7158450063671093\n",
      "R^2 Score: -0.012906771085273716\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2230\n",
      "           1       0.28      1.00      0.44      1562\n",
      "           2       0.00      0.00      0.00      1705\n",
      "\n",
      "    accuracy                           0.28      5497\n",
      "   macro avg       0.09      0.33      0.15      5497\n",
      "weighted avg       0.08      0.28      0.13      5497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Tokenization and Padding\n",
    "MAX_LEN = 150\n",
    "VOCAB_SIZE = 10000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(tweets_df['Processed_Text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets_df['Processed_Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "\n",
    "# LSTM model\n",
    "model_lstm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64, input_length=MAX_LEN),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n",
    "X = padded_sequences\n",
    "y = tweets_df['Sentiment'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_lstm.fit(X, y, epochs=5, validation_split=0.2, batch_size=32)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, classification_report\n",
    "\n",
    "# Predictions\n",
    "with tf.device('/CPU:0'):\n",
    "    y_pred_lstm = model_lstm.predict(X_test).flatten()\n",
    "y_pred_class = [1 if i > 0.5 else 0 for i in y_pred_lstm]\n",
    "\n",
    "# Metrics\n",
    "accuracy = sum(y_pred_class == y_test) / len(y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_class)\n",
    "r2 = r2_score(y_test, y_pred_class)\n",
    "class_report = classification_report(y_test, y_pred_class)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Change number of labels to 3\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tweets_df['Processed_Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True,\n",
    "                                         return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = [t.numpy()[0] for t in input_ids]\n",
    "attention_masks = [t.numpy()[0] for t in attention_masks]\n",
    "\n",
    "X_ids = tf.convert_to_tensor(input_ids)\n",
    "X_masks = tf.convert_to_tensor(attention_masks)\n",
    "y = tf.convert_to_tensor(tweets_df['Sentiment'].values)\n",
    "X_ids_np = X_ids.numpy()\n",
    "X_masks_np = X_masks.numpy()\n",
    "y_np = y.numpy() \n",
    "\n",
    "X_ids_train, X_ids_test, X_masks_train, X_masks_test, y_train, y_test = train_test_split(X_ids_np, X_masks_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # Change to CategoricalCrossentropy\n",
    "metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')  # Change metric for multiclass\n",
    "\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "y_one_hot_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "model_bert.fit([X_ids_train, X_masks_train], y_one_hot_train, epochs=1, validation_split=0.2, batch_size=8)\n",
    "\n",
    "# Predictions\n",
    "y_pred_bert = model_bert.predict([X_ids_test, X_masks_test])[0]\n",
    "y_pred_bert_class = np.argmax(y_pred_bert, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = sum(y_pred_bert_class == y_test) / len(y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_bert_class)\n",
    "r2 = r2_score(y_test, y_pred_bert_class)\n",
    "class_report = classification_report(y_test, y_pred_bert_class)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 3)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_balanced = tweets_df\n",
    "positive_reviews_tweet = tweet_df_balanced[tweet_df_balanced['Sentiment'] == 2].sample(500, random_state=42)\n",
    "positive_reviews_tweet = tweet_df_balanced[tweet_df_balanced['Sentiment'] == 1].sample(500, random_state=42)\n",
    "neutral_reviews_tweet = tweet_df_balanced[tweet_df_balanced['Sentiment'] == 0].sample(500, random_state=42)\n",
    "# Combine positive and negative reviews\n",
    "tweet_df_balanced = pd.concat([positive_reviews_tweet, positive_reviews_tweet,neutral_reviews_tweet])\n",
    "# Shuffle the rows\n",
    "tweet_df_balanced = tweet_df_balanced.sample(frac=1, random_state=42)\n",
    "tweet_df_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "120/120 [==============================] - 1461s 12s/step - loss: 0.6695 - accuracy: 0.6625 - val_loss: 0.5360 - val_accuracy: 0.7375\n",
      "Epoch 2/2\n",
      "120/120 [==============================] - 1444s 12s/step - loss: 0.4540 - accuracy: 0.8083 - val_loss: 0.4387 - val_accuracy: 0.8000\n",
      "10/10 [==============================] - 146s 14s/step\n",
      "Accuracy: 0.81\n",
      "Mean Absolute Error: 0.19\n",
      "R^2 Score: 0.10639632107023411\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.67        92\n",
      "           1       0.85      0.88      0.87       208\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.78      0.76      0.77       300\n",
      "weighted avg       0.81      0.81      0.81       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Change number of labels to 3\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tweet_df_balanced['Processed_Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True,\n",
    "                                         return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = [t.numpy()[0] for t in input_ids]\n",
    "attention_masks = [t.numpy()[0] for t in attention_masks]\n",
    "\n",
    "X_ids = tf.convert_to_tensor(input_ids)\n",
    "X_masks = tf.convert_to_tensor(attention_masks)\n",
    "y = tf.convert_to_tensor(tweet_df_balanced['Sentiment'].values)\n",
    "X_ids_np = X_ids.numpy()\n",
    "X_masks_np = X_masks.numpy()\n",
    "y_np = y.numpy() \n",
    "\n",
    "X_ids_train, X_ids_test, X_masks_train, X_masks_test, y_train, y_test = train_test_split(X_ids_np, X_masks_np, y_np, test_size=0.2, random_state=42)\n",
    "X_ids_train = tf.cast(X_ids_train, tf.int32)\n",
    "X_masks_train = tf.cast(X_masks_train, tf.int32)\n",
    "y_train = tf.cast(y_train, tf.int64)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # Change to CategoricalCrossentropy\n",
    "metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')  # Change metric for multiclass\n",
    "\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "y_one_hot_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "model_bert.fit([X_ids_train, X_masks_train], y_one_hot_train, epochs=2, validation_split=0.2, batch_size=8)\n",
    "\n",
    "# Predictions\n",
    "y_pred_bert = model_bert.predict([X_ids_test, X_masks_test])[0]\n",
    "y_pred_bert_class = np.argmax(y_pred_bert, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = sum(y_pred_bert_class == y_test) / len(y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_bert_class)\n",
    "r2 = r2_score(y_test, y_pred_bert_class)\n",
    "class_report = classification_report(y_test, y_pred_bert_class)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/25 [==============================] - 5s 109ms/step - loss: 1.0974 - accuracy: 0.3742 - val_loss: 1.0899 - val_accuracy: 0.4200\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 3s 103ms/step - loss: 1.0272 - accuracy: 0.6408 - val_loss: 0.9925 - val_accuracy: 0.5550\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 3s 103ms/step - loss: 0.7367 - accuracy: 0.7209 - val_loss: 0.9797 - val_accuracy: 0.5300\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 3s 103ms/step - loss: 0.4991 - accuracy: 0.8836 - val_loss: 1.1196 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 3s 103ms/step - loss: 0.4837 - accuracy: 0.8411 - val_loss: 1.1227 - val_accuracy: 0.4500\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 1.1227 - accuracy: 0.4500\n",
      "Accuracy: 45.00%\n",
      "7/7 [==============================] - 0s 16ms/step\n",
      "LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.30      0.34        66\n",
      "     neutral       0.43      0.61      0.50        67\n",
      "    positive       0.56      0.43      0.49        67\n",
      "\n",
      "    accuracy                           0.45       200\n",
      "   macro avg       0.46      0.45      0.44       200\n",
      "weighted avg       0.46      0.45      0.44       200\n",
      "\n",
      "Mean Absolute Error: 0.695\n",
      "R2 Score: -0.4812586939358623\n"
     ]
    }
   ],
   "source": [
    "# First, ensure you have the necessary libraries installed\n",
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Build the LSTM model\n",
    "max_words = 5000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(X_train_balanced)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train_balanced)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test_balanced)\n",
    "\n",
    "X_train_seq = pad_sequences(sequences_train, maxlen=max_length)\n",
    "X_test_seq = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "# Convert labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(y_train_balanced)\n",
    "y_test_int = label_encoder.transform(y_test_balanced)\n",
    "\n",
    "# Convert integer labels to binary class matrix\n",
    "y_train_cat = to_categorical(y_train_int)\n",
    "y_test_cat = to_categorical(y_test_int)\n",
    "\n",
    "X_train_seq.shape, y_train_cat.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_seq, y_train_cat, validation_data=(X_test_seq, y_test_cat), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_seq, y_test_cat)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "y_pred = model.predict(X_test_seq)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"LSTM Classification Report:\")\n",
    "print(classification_report(y_test_int, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Mean Absolute Error and R2 Score\n",
    "mae = mean_absolute_error(y_test_int, y_pred_classes)\n",
    "r2 = r2_score(y_test_int, y_pred_classes)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 53), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 53), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 53), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "<_BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 59), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 59), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 59), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model of News Dataset!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
